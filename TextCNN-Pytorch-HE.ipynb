{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the size of train: 65328, dev:872, test:2021\n",
      "train.fields: {'text': <torchtext.data.field.Field object at 0x00000283AD246F10>, 'label': <torchtext.data.field.Field object at 0x00000283AD246EB0>} torch.Size([14795, 50])\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "TEXT = data.Field(lower=True, fix_length=50, batch_first=True)\n",
    "LABEL = data.Field(sequential=False,)\n",
    "\n",
    "train, dev, test = data.TabularDataset.splits(\n",
    "    path='SST-2', train='train.tsv', validation='dev.tsv',\n",
    "    test='test.tsv', format='tsv', skip_header=True,\n",
    "    fields=[('text', TEXT), ('label', LABEL)])\n",
    "print(\"the size of train: {}, dev:{}, test:{}\".format(len(train.examples), len(dev.examples), len(test.examples)))\n",
    "\n",
    "TEXT.build_vocab(train, vectors=GloVe(name='6B', dim=50), max_size=25000)\n",
    "LABEL.build_vocab(train,)\n",
    "\n",
    "print(\"train.fields:\", train.fields, TEXT.vocab.vectors.shape)\n",
    "\n",
    "train_iter, dev_iter, test_iter = data.BucketIterator.splits(\n",
    "        (train, dev, test), batch_sizes=(64, 64, 64), sort_key=lambda x: len(x.text), sort_within_batch=True, repeat=False\n",
    "    )\n",
    "train_iter.repeat = False\n",
    "test_iter.repeat = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text-CNN Parameter\n",
    "sequence_length = 50\n",
    "vocab_size = TEXT.vocab.vectors.shape[0]\n",
    "embedding_size = TEXT.vocab.vectors.shape[1]\n",
    "num_classes = 2  # 0 or 1\n",
    "filter_sizes = [2, 3, 5] # n-gram window\n",
    "num_filters = 2\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tenseal as ts\n",
    "\n",
    "# Create TenSEAL context\n",
    "context_client = ts.context(\n",
    "    ts.SCHEME_TYPE.CKKS, 16384, coeff_mod_bit_sizes=[ 58, 40, 40, 40, 40, 40, 40, 40, 40, 58 ]\n",
    ")\n",
    "# set the scale\n",
    "context_client.global_scale = pow(2, 40)\n",
    "# generated galois keys in order to do rotation on ciphertext vectors\n",
    "context_client.generate_galois_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class Sigmoid(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = 0.5 + 0.197*x - 0.004*torch.pow(x, 3)\n",
    "        return x\n",
    "\n",
    "class Softmax(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Softmax, self).__init__()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = 0.25 + 0.5*x + 0.125*torch.pow(x, 2)\n",
    "        return x\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Swish, self).__init__()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = 0.1198 + 0.5*x + 0.1473*torch.pow(x, 2) - 0.002012*torch.pow(x, 4)\n",
    "        return x\n",
    "\n",
    "def sigmoid(ckks_vec):\n",
    "    return ckks_vec.polyval([0.5, 0.197, 0, -0.004])\n",
    "    \n",
    "def softmax(ckks_vec):\n",
    "    return ckks_vec.polyval([0.25, 0.5, 0.125])\n",
    "\n",
    "def swish(ckks_vec):\n",
    "    return ckks_vec.polyval([0.1198, 0.5, 0.1473, 0, -0.002012])\n",
    "\n",
    "class HETextCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HETextCNN, self).__init__()\n",
    "\n",
    "        self.num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(1, num_filters, (kernel, embedding_size), bias=False),\n",
    "                Swish(),\n",
    "                nn.AvgPool2d((sequence_length - kernel + 1,1))\n",
    "            ) for kernel in filter_sizes])\n",
    "        \n",
    "        self.fc = nn.Linear(self.num_filters_total,num_classes)\n",
    "        self.sm = Softmax()\n",
    "                           \n",
    "    def forward(self, X):\n",
    "        embedded_chars = self.embedding(X)# [batch_size, sequence_length, embedding_size]\n",
    "        embedded_chars = embedded_chars.unsqueeze(1)\n",
    "\n",
    "        # Plain forward\n",
    "        plain_out = [conv(embedded_chars) for conv in self.convs]\n",
    "        plain_out = torch.cat(plain_out, dim=1)\n",
    "        plain_out = plain_out.view(embedded_chars.size(0), -1)\n",
    "        plain_out = self.fc(plain_out)\n",
    "        print(plain_out)\n",
    "        plain_logit = self.sm(plain_out)\n",
    "        \n",
    "        # Cipher forward\n",
    "        cipher_logits = []\n",
    "        for single in embedded_chars:\n",
    "            # Encrypt and encode\n",
    "            x_enc1, windows_nb1 = ts.im2col_encoding(context_client, single[0][:26].tolist(), 2, embedding_size, 1)\n",
    "            x_enc2, windows_nb2 = ts.im2col_encoding(context_client, single[0][25:].tolist(), 2, embedding_size, 1)\n",
    "\n",
    "            x_enc3, windows_nb3 = ts.im2col_encoding(context_client, single[0][:27].tolist(), 3, embedding_size, 1)\n",
    "            x_enc4, windows_nb4 = ts.im2col_encoding(context_client, single[0][25:].tolist(), 3, embedding_size, 1)\n",
    "\n",
    "            x_enc5, windows_nb5 = ts.im2col_encoding(context_client, single[0][:27].tolist(), 5, embedding_size, 1)\n",
    "            x_enc6, windows_nb6 = ts.im2col_encoding(context_client, single[0][23:].tolist(), 5, embedding_size, 1)\n",
    "\n",
    "            cipher_out = []\n",
    "            fc_weight = self.fc._parameters['weight'].clone().T\n",
    "\n",
    "            for idx in range(len(self.convs)):\n",
    "                conv_weights = self.convs[idx][0]._parameters['weight'].tolist()\n",
    "                for channel in range(num_filters):\n",
    "                    kernel = conv_weights[channel][0]\n",
    "\n",
    "                    if len(kernel) == 2: # 2-gram, 25+24\n",
    "                        c_conv1 = x_enc1.conv2d_im2col(kernel, windows_nb1)\n",
    "                        c_conv2 = x_enc2.conv2d_im2col(kernel, windows_nb2)\n",
    "                        \n",
    "                        h1 = swish(c_conv1)\n",
    "                        h2 = swish(c_conv2)\n",
    "                        ap = h1.sum() + h2.sum() # [1, sequence_length - filter_size + 1]\n",
    "\n",
    "                        cipher_out.append(ap)\n",
    "                        print(\"Conv out: \", ap.decrypt())\n",
    "                    elif len(kernel) == 3: # 3-gram, 25+23\n",
    "                        c_conv1 = x_enc3.conv2d_im2col(kernel, windows_nb3)\n",
    "                        c_conv2 = x_enc4.conv2d_im2col(kernel, windows_nb4)\n",
    "\n",
    "                        h1 = swish(c_conv1)\n",
    "                        h2 = swish(c_conv2)\n",
    "                        ap = h1.sum() + h2.sum() # [1, sequence_length - filter_size + 1]\n",
    "\n",
    "                        cipher_out.append(ap)\n",
    "                        print(\"Conv out: \", ap.decrypt())\n",
    "                    elif len(kernel) == 5: # 5-gram, 23+23\n",
    "                        c_conv1 = x_enc5.conv2d_im2col(kernel, windows_nb5)\n",
    "                        c_conv2 = x_enc6.conv2d_im2col(kernel, windows_nb6)\n",
    "\n",
    "                        h1 = swish(c_conv1)\n",
    "                        h2 = swish(c_conv2)\n",
    "                        ap = h1.sum() + h2.sum() # [1, sequence_length - filter_size + 1]\n",
    "\n",
    "                        cipher_out.append(ap)\n",
    "                        print(\"Conv out: \", ap.decrypt())\n",
    "\n",
    "                    fc_weight[idx * num_filters + channel] /= sequence_length - len(kernel) + 1\n",
    "\n",
    "            cipher_out = ts.pack_vectors(cipher_out)\n",
    "\n",
    "            fc_bias = self.fc._parameters['bias']\n",
    "            cipher_out = cipher_out.mm_(fc_weight.tolist()) + fc_bias.tolist() # [1, num_classes]\n",
    "\n",
    "            print(\"FC out: \", cipher_out.decrypt())\n",
    "            cipher_logit = softmax(cipher_out)\n",
    "\n",
    "            cipher_logits.append(cipher_logit.decrypt())\n",
    "            print(\"Softmax out: \", cipher_logit.decrypt())\n",
    "        \n",
    "        acc_loss = (np.abs(np.array(cipher_logits) - np.array(plain_logit.tolist())) / np.array(plain_logit.tolist())).sum()\n",
    "        \n",
    "        print(\"Batch acc loss: \", acc_loss)\n",
    "        \n",
    "        return plain_logit\n",
    "    \n",
    "    def forward_plain(self, X):\n",
    "        embedded_chars = self.embedding(X)# [batch_size, sequence_length, sequence_length]\n",
    "        embedded_chars = embedded_chars.unsqueeze(1)\n",
    "\n",
    "        out = [conv(embedded_chars) for conv in self.convs]\n",
    "        out = torch.cat(out, dim=1)\n",
    "        out = out.view(embedded_chars.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        logit = self.sm(out)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(preds, y):\n",
    "    correct = torch.eq(preds, y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "train_set_size = 128\n",
    "\n",
    "def train(model, optimizer, criterion):\n",
    "    avg_acc = []\n",
    "    avg_loss = []\n",
    "    model.train()\n",
    "    for batch_idx , batch in enumerate(train_iter):\n",
    "        if batch_idx >= train_set_size:\n",
    "            continue\n",
    "        text, labels = batch.text , batch.label - 1\n",
    "        predicted = model.forward_plain(text)\n",
    "\n",
    "        acc = binary_acc(torch.max(predicted, dim=1)[1], labels)\n",
    "        avg_acc.append(acc)\n",
    "        loss = criterion(predicted, labels)\n",
    "        avg_loss.append(loss)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return np.array(avg_acc).mean()\n",
    "\n",
    "def evaluate(model, criterion):\n",
    "    avg_acc = []\n",
    "    model.eval()\n",
    "    for batch_idx , batch in enumerate(dev_iter):\n",
    "        text, labels = batch.text , batch.label - 1\n",
    "        predicted = model(text)\n",
    "\n",
    "        acc = binary_acc(torch.max(predicted, dim=1)[1], labels)\n",
    "        avg_acc.append(acc)\n",
    "\n",
    "    return np.array(avg_acc).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HETextCNN(\n",
      "  (embedding): Embedding(14795, 50)\n",
      "  (convs): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1, 2, kernel_size=(2, 50), stride=(1, 1), bias=False)\n",
      "      (1): Swish()\n",
      "      (2): AvgPool2d(kernel_size=(49, 1), stride=(49, 1), padding=0)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(1, 2, kernel_size=(3, 50), stride=(1, 1), bias=False)\n",
      "      (1): Swish()\n",
      "      (2): AvgPool2d(kernel_size=(48, 1), stride=(48, 1), padding=0)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(1, 2, kernel_size=(5, 50), stride=(1, 1), bias=False)\n",
      "      (1): Swish()\n",
      "      (2): AvgPool2d(kernel_size=(46, 1), stride=(46, 1), padding=0)\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=6, out_features=2, bias=True)\n",
      "  (sm): Softmax()\n",
      ")\n",
      "epoch=0,训练准确率=0.5623779296875\n",
      "epoch=1,训练准确率=0.6224365234375\n",
      "epoch=2,训练准确率=0.685546875\n",
      "epoch=3,训练准确率=0.7508544921875\n",
      "epoch=4,训练准确率=0.8013916015625\n",
      "epoch=5,训练准确率=0.820556640625\n",
      "epoch=6,训练准确率=0.849365234375\n",
      "epoch=7,训练准确率=0.8629150390625\n",
      "epoch=8,训练准确率=0.87548828125\n",
      "epoch=9,训练准确率=0.87939453125\n",
      "epoch=10,训练准确率=0.88671875\n",
      "epoch=11,训练准确率=0.8841552734375\n",
      "epoch=12,训练准确率=0.89013671875\n",
      "epoch=13,训练准确率=0.9019775390625\n",
      "epoch=14,训练准确率=0.91162109375\n",
      "epoch=15,训练准确率=0.8990478515625\n",
      "epoch=16,训练准确率=0.9127197265625\n",
      "epoch=17,训练准确率=0.9090576171875\n",
      "epoch=18,训练准确率=0.916015625\n",
      "epoch=19,训练准确率=0.9193115234375\n",
      "epoch=20,训练准确率=0.9169921875\n",
      "epoch=21,训练准确率=0.918701171875\n",
      "epoch=22,训练准确率=0.9222412109375\n",
      "epoch=23,训练准确率=0.91943359375\n",
      "epoch=24,训练准确率=0.91650390625\n",
      "epoch=25,训练准确率=0.930908203125\n",
      "epoch=26,训练准确率=0.9208984375\n",
      "epoch=27,训练准确率=0.9210205078125\n",
      "epoch=28,训练准确率=0.9349365234375\n",
      "epoch=29,训练准确率=0.93408203125\n",
      "epoch=30,训练准确率=0.9378662109375\n",
      "epoch=31,训练准确率=0.940673828125\n",
      "epoch=32,训练准确率=0.936767578125\n",
      "epoch=33,训练准确率=0.9388427734375\n",
      "epoch=34,训练准确率=0.940673828125\n",
      "epoch=35,训练准确率=0.945068359375\n",
      "epoch=36,训练准确率=0.940185546875\n",
      "epoch=37,训练准确率=0.9454345703125\n",
      "epoch=38,训练准确率=0.947265625\n",
      "epoch=39,训练准确率=0.94580078125\n",
      "epoch=40,训练准确率=0.946044921875\n",
      "epoch=41,训练准确率=0.945068359375\n",
      "epoch=42,训练准确率=0.939697265625\n",
      "epoch=43,训练准确率=0.945068359375\n",
      "epoch=44,训练准确率=0.948974609375\n",
      "epoch=45,训练准确率=0.9468994140625\n",
      "epoch=46,训练准确率=0.9444580078125\n",
      "epoch=47,训练准确率=0.947509765625\n",
      "epoch=48,训练准确率=0.9495849609375\n",
      "epoch=49,训练准确率=0.9539794921875\n",
      "tensor([[ 8.6558, 10.3380],\n",
      "        [ 9.6042, 10.1499],\n",
      "        [10.5630,  8.9516],\n",
      "        [11.0026,  8.3804],\n",
      "        [ 8.9375, 10.8333],\n",
      "        [ 9.0648, 11.0140],\n",
      "        [10.4710,  8.5993],\n",
      "        [ 8.9725, 10.6627],\n",
      "        [ 8.6378, 11.3665],\n",
      "        [ 9.5951,  9.8803],\n",
      "        [ 9.2269, 10.9472],\n",
      "        [ 9.0989, 10.6001],\n",
      "        [10.2410,  9.7840],\n",
      "        [ 8.8133, 10.4538],\n",
      "        [10.9895,  8.0643],\n",
      "        [10.7032,  9.2270],\n",
      "        [ 9.5253, 10.3041],\n",
      "        [11.0331,  8.1444],\n",
      "        [ 9.6566, 10.2465],\n",
      "        [ 9.3683, 10.5001],\n",
      "        [ 9.7783,  9.9058],\n",
      "        [10.6317,  8.9755],\n",
      "        [ 9.3553, 10.4087],\n",
      "        [ 9.4945, 10.6404],\n",
      "        [10.8026,  8.5831],\n",
      "        [ 9.5890, 10.1385],\n",
      "        [10.9968,  8.3544],\n",
      "        [ 9.3616, 10.3640],\n",
      "        [10.4773,  8.7275],\n",
      "        [ 9.4069, 10.5604],\n",
      "        [ 9.1888, 10.5250],\n",
      "        [ 8.9365, 10.8990],\n",
      "        [ 9.8518, 10.2507],\n",
      "        [ 9.1422, 10.5670],\n",
      "        [10.0469,  9.7895],\n",
      "        [10.8409,  8.9935],\n",
      "        [ 9.5133, 10.2728],\n",
      "        [10.6360,  8.7067],\n",
      "        [10.4618,  8.8293],\n",
      "        [10.3541,  9.7039],\n",
      "        [ 9.2859, 10.6813],\n",
      "        [10.8349,  8.9377],\n",
      "        [10.5751,  9.1651],\n",
      "        [ 9.3859, 10.6880],\n",
      "        [10.8197,  9.0299],\n",
      "        [10.4012,  9.4917],\n",
      "        [ 8.9333,  9.7312],\n",
      "        [ 9.5739, 10.3246],\n",
      "        [ 9.4333, 10.7719],\n",
      "        [ 9.4545, 10.7757],\n",
      "        [10.1620,  9.6594],\n",
      "        [10.8968,  8.6490],\n",
      "        [ 8.8351, 10.7933],\n",
      "        [ 9.7719, 10.4713],\n",
      "        [ 9.1688, 10.7649],\n",
      "        [ 9.6864, 10.4858],\n",
      "        [ 9.4506, 10.4346],\n",
      "        [10.7361,  9.0976],\n",
      "        [ 9.8768, 10.3257],\n",
      "        [10.3247,  9.8674],\n",
      "        [ 9.1362, 10.9745],\n",
      "        [ 9.5940, 10.6458],\n",
      "        [10.1694,  9.9570],\n",
      "        [10.0338, 10.3251]], grad_fn=<AddmmBackward>)\n",
      "Conv out:  [28.759679394811588]\n",
      "Conv out:  [88.46716507958689]\n",
      "Conv out:  [232.0905179969643]\n",
      "Conv out:  [177.2185834412491]\n",
      "Conv out:  [267.20797455151035]\n",
      "Conv out:  [245.8114197535104]\n",
      "FC out:  [8.655925853864664, 10.33807256513542]\n",
      "Softmax out:  [13.943748496695866, 18.778717571512342]\n",
      "Conv out:  [23.778447956424603]\n",
      "Conv out:  [83.42063022972262]\n",
      "Conv out:  [243.06133933141896]\n",
      "Conv out:  [192.09400430786386]\n",
      "Conv out:  [263.8843433412765]\n",
      "Conv out:  [267.5068326403527]\n",
      "FC out:  [9.604346948638787, 10.149998949615272]\n",
      "Softmax out:  [16.582794752788722, 18.203015452078272]\n",
      "Conv out:  [14.23558199007365]\n",
      "Conv out:  [73.03849383106407]\n",
      "Conv out:  [255.65218125318322]\n",
      "Conv out:  [202.36506056435846]\n",
      "Conv out:  [256.3448721623012]\n",
      "Conv out:  [252.58013663231498]\n",
      "FC out:  [10.563151151162323, 8.951736180028565]\n",
      "Softmax out:  [19.479317727421467, 14.742729418616847]\n",
      "Conv out:  [11.697877543092094]\n",
      "Conv out:  [71.3615126181254]\n",
      "Conv out:  [262.869392513027]\n",
      "Conv out:  [209.14416144519683]\n",
      "Conv out:  [250.4894906265824]\n",
      "Conv out:  [241.98640276425644]\n",
      "FC out:  [11.002746949557126, 8.380426220764473]\n",
      "Softmax out:  [20.88416779285695, 13.219301299650965]\n",
      "Conv out:  [30.039277487503824]\n",
      "Conv out:  [88.99527759815744]\n",
      "Conv out:  [236.6972763907301]\n",
      "Conv out:  [182.63341675238502]\n",
      "Conv out:  [269.4925780337245]\n",
      "Conv out:  [271.5420733578867]\n",
      "FC out:  [8.93756713671607, 10.833393944353505]\n",
      "Softmax out:  [14.703960128836869, 20.33723250751033]\n",
      "Conv out:  [33.34480691282534]\n",
      "Conv out:  [96.12081884578369]\n",
      "Conv out:  [245.84955697940632]\n",
      "Conv out:  [187.37435433438014]\n",
      "Conv out:  [273.984983520298]\n",
      "Conv out:  [267.20781885313625]\n",
      "FC out:  [9.064903369277099, 11.014063450681503]\n",
      "Softmax out:  [15.054178369331511, 20.92097065667997]\n",
      "Conv out:  [12.993789395786338]\n",
      "Conv out:  [70.99433563553484]\n",
      "Conv out:  [252.49632783999488]\n",
      "Conv out:  [199.01947512406304]\n",
      "Conv out:  [248.2554405660429]\n",
      "Conv out:  [244.66365330687432]\n",
      "FC out:  [10.471096912378622, 8.599418733538077]\n",
      "Softmax out:  [19.191250596545313, 13.793611886162749]\n",
      "Conv out:  [27.006387828959483]\n",
      "Conv out:  [86.87219444387522]\n",
      "Conv out:  [236.08946433776998]\n",
      "Conv out:  [180.4545809683985]\n",
      "Conv out:  [268.9495881013539]\n",
      "Conv out:  [269.4424370477308]\n",
      "FC out:  [8.972625440196403, 10.662831012404752]\n",
      "Softmax out:  [14.799978083728243, 19.793636922871737]\n",
      "Conv out:  [35.04108264337336]\n",
      "Conv out:  [94.30936977023133]\n",
      "Conv out:  [235.51822001363962]\n",
      "Conv out:  [180.5638284153075]\n",
      "Conv out:  [274.80664697104777]\n",
      "Conv out:  [276.7749339489365]\n",
      "FC out:  [8.63795411931768, 11.366646942441006]\n",
      "Softmax out:  [13.89591192675835, 22.083660448829374]\n",
      "Conv out:  [19.59936488527584]\n",
      "Conv out:  [78.3384937827471]\n",
      "Conv out:  [242.27192096144748]\n",
      "Conv out:  [186.4423176515956]\n",
      "Conv out:  [264.18246938848847]\n",
      "Conv out:  [262.66718151720283]\n",
      "FC out:  [9.595261973373638, 9.88039389484057]\n",
      "Softmax out:  [16.556448435805045, 17.393166066701262]\n",
      "Conv out:  [28.58175174582863]\n",
      "Conv out:  [91.07671603886818]\n",
      "Conv out:  [243.50998149817278]\n",
      "Conv out:  [187.2712712741012]\n",
      "Conv out:  [275.2966951949472]\n",
      "Conv out:  [275.9726630810667]\n",
      "FC out:  [9.22700938709655, 10.94733355627559]\n",
      "Softmax out:  [15.505890529363588, 20.704417813276134]\n",
      "Conv out:  [32.528450537511794]\n",
      "Conv out:  [93.84800784423842]\n",
      "Conv out:  [243.89129806790325]\n",
      "Conv out:  [189.53970509551618]\n",
      "Conv out:  [271.22297364927607]\n",
      "Conv out:  [253.41663851333786]\n",
      "FC out:  [9.09901516236406, 10.600183161314195]\n",
      "Softmax out:  [15.148685880525482, 19.595800283198187]\n",
      "Conv out:  [16.63756959311971]\n",
      "Conv out:  [79.3487425409024]\n",
      "Conv out:  [253.94982526875665]\n",
      "Conv out:  [197.48832792365687]\n",
      "Conv out:  [267.8869524776313]\n",
      "Conv out:  [267.8784344453421]\n",
      "FC out:  [10.24114874198679, 9.78405231727534]\n",
      "Softmax out:  [18.480924882807308, 17.108178812838094]\n",
      "Conv out:  [33.034155212751294]\n",
      "Conv out:  [94.99615874516816]\n",
      "Conv out:  [240.10738246618604]\n",
      "Conv out:  [183.20233084224498]\n",
      "Conv out:  [264.89336421215177]\n",
      "Conv out:  [245.27639660597956]\n",
      "FC out:  [8.813410343220095, 10.453862703424681]\n",
      "Softmax out:  [14.366389566323205, 19.137554694504075]\n",
      "Conv out:  [10.518666537432168]\n",
      "Conv out:  [67.15982514039763]\n",
      "Conv out:  [259.4899121713855]\n",
      "Conv out:  [207.07997451792102]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv out:  [241.31054305925278]\n",
      "Conv out:  [240.60921679208832]\n",
      "FC out:  [10.989668859633522, 8.064369770967605]\n",
      "Softmax out:  [20.841675878769692, 12.411577827586537]\n",
      "Conv out:  [15.299580379084098]\n",
      "Conv out:  [79.04524312773368]\n",
      "Conv out:  [260.7805638984521]\n",
      "Conv out:  [206.8358611631728]\n",
      "Conv out:  [259.9873287022399]\n",
      "Conv out:  [258.06338379563107]\n",
      "FC out:  [10.703336891990372, 9.227088023979746]\n",
      "Softmax out:  [19.922073375957034, 15.506111242767014]\n",
      "Conv out:  [27.8960715075199]\n",
      "Conv out:  [87.48036986732536]\n",
      "Conv out:  [247.14709316149563]\n",
      "Conv out:  [192.0372571787078]\n",
      "Conv out:  [263.3501185147921]\n",
      "Conv out:  [265.31944708287347]\n",
      "FC out:  [9.525391745434344, 10.304224352510236]\n",
      "Softmax out:  [16.35451530661136, 18.67445406607104]\n",
      "Conv out:  [10.846427719988965]\n",
      "Conv out:  [67.99573023650994]\n",
      "Conv out:  [261.74150102915206]\n",
      "Conv out:  [208.8743915610586]\n",
      "Conv out:  [247.70728568075458]\n",
      "Conv out:  [237.8333465284514]\n",
      "FC out:  [11.033250252130054, 8.144499661852915]\n",
      "Softmax out:  [20.98344200170944, 12.613997081855224]\n",
      "Conv out:  [26.988036980526267]\n",
      "Conv out:  [86.86223847036867]\n",
      "Conv out:  [249.28286895065432]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-c5b5fcfeb4e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'epoch={},训练准确率={}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"epoch={},测试准确率={}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-ec9ba409f65e>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(model, criterion)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbinary_acc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\syft\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-9293eb907e49>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     98\u001b[0m                         \u001b[0mc_conv2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_enc4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d_im2col\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindows_nb4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m                         \u001b[0mh1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mswish\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_conv1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m                         \u001b[0mh2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mswish\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_conv2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m                         \u001b[0map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mh2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# [1, sequence_length - filter_size + 1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = HETextCNN()\n",
    "print(model)\n",
    "\n",
    "pretrained_embedding = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embedding)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_accs, test_accs = [], []\n",
    "\n",
    "for epoch in range(50):\n",
    "\n",
    "    train_acc = train(model, optimizer, criterion)\n",
    "    print('epoch={},训练准确率={}'.format(epoch, train_acc))\n",
    "test_acc = evaluate(model, criterion)\n",
    "print(\"epoch={},测试准确率={}\".format(epoch, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

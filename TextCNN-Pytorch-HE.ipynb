{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the size of train: 65328, dev:872, test:2021\n",
      "train.fields: {'text': <torchtext.data.field.Field object at 0x000001EC22D89FD0>, 'label': <torchtext.data.field.Field object at 0x000001EC22D89F70>} torch.Size([14795, 50])\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "TEXT = data.Field(lower=True, fix_length=50, batch_first=True)\n",
    "LABEL = data.Field(sequential=False,)\n",
    "\n",
    "train, dev, test = data.TabularDataset.splits(\n",
    "    path='SST-2', train='train.tsv', validation='dev.tsv',\n",
    "    test='test.tsv', format='tsv', skip_header=True,\n",
    "    fields=[('text', TEXT), ('label', LABEL)])\n",
    "print(\"the size of train: {}, dev:{}, test:{}\".format(len(train.examples), len(dev.examples), len(test.examples)))\n",
    "\n",
    "TEXT.build_vocab(train, vectors=GloVe(name='6B', dim=50), max_size=25000)\n",
    "LABEL.build_vocab(train,)\n",
    "\n",
    "print(\"train.fields:\", train.fields, TEXT.vocab.vectors.shape)\n",
    "\n",
    "train_iter, dev_iter, test_iter = data.BucketIterator.splits(\n",
    "        (train, dev, test), batch_sizes=(32, 32, 32), sort_key=lambda x: len(x.text), sort_within_batch=True, repeat=False\n",
    "    )\n",
    "train_iter.repeat = False\n",
    "test_iter.repeat = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text-CNN Parameter\n",
    "sequence_length = 50\n",
    "vocab_size = TEXT.vocab.vectors.shape[0]\n",
    "embedding_size = TEXT.vocab.vectors.shape[1]\n",
    "num_classes = 2  # 0 or 1\n",
    "filter_sizes = [2, 3, 5] # n-gram window\n",
    "num_filters = 2\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tenseal as ts\n",
    "\n",
    "# Create TenSEAL context\n",
    "context_client = ts.context(\n",
    "    ts.SCHEME_TYPE.CKKS, 16384, coeff_mod_bit_sizes=[ 58, 40, 40, 40, 40, 40, 40, 40, 40, 58 ]\n",
    ")\n",
    "# set the scale\n",
    "context_client.global_scale = pow(2, 40)\n",
    "# generated galois keys in order to do rotation on ciphertext vectors\n",
    "context_client.generate_galois_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class Sigmoid(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = 0.5 + 0.197*x - 0.004*torch.pow(x, 3)\n",
    "        return x\n",
    "\n",
    "class Softmax(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Softmax, self).__init__()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = 1 + x + 0.5*torch.pow(x, 2)\n",
    "        return x\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Swish, self).__init__()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = 0.1198 + 0.5*x + 0.1473*torch.pow(x, 2) - 0.002012*torch.pow(x, 4)\n",
    "        return x\n",
    "\n",
    "def sigmoid(ckks_vec):\n",
    "    return ckks_vec.polyval([0.5, 0.197, 0, -0.004])\n",
    "    \n",
    "def softmax(ckks_vec):\n",
    "    return ckks_vec.polyval([1, 1, 0.5])\n",
    "\n",
    "def swish(ckks_vec):\n",
    "    return ckks_vec.polyval([0.1198, 0.5, 0.1473, 0, -0.002012])\n",
    "\n",
    "class HETextCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HETextCNN, self).__init__()\n",
    "\n",
    "        self.num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(1, num_filters, (kernel, embedding_size), bias=False),\n",
    "                Swish(),\n",
    "                nn.AvgPool2d((sequence_length - kernel + 1,1))\n",
    "            ) for kernel in filter_sizes])\n",
    "        \n",
    "        self.fc = nn.Linear(self.num_filters_total,num_classes)\n",
    "        self.sm = Softmax()\n",
    "                           \n",
    "    def forward(self, X):\n",
    "        embedded_chars = self.embedding(X)# [batch_size, sequence_length, embedding_size]\n",
    "        embedded_chars = embedded_chars.unsqueeze(1)\n",
    "\n",
    "        # Plain forward\n",
    "        plain_out = [conv(embedded_chars) for conv in self.convs]\n",
    "        plain_out = torch.cat(plain_out, dim=1)\n",
    "        plain_out = plain_out.view(embedded_chars.size(0), -1)\n",
    "        plain_out = self.fc(plain_out)\n",
    "        print(plain_out)\n",
    "        plain_logit = self.sm(plain_out)\n",
    "        \n",
    "        # Cipher forward\n",
    "        cipher_logits = []\n",
    "        for single in embedded_chars:\n",
    "            # Encrypt and encode\n",
    "            x_enc1, windows_nb1 = ts.im2col_encoding(context_client, single[0][:26].tolist(), 2, embedding_size, 1) #25\n",
    "            x_enc2, windows_nb2 = ts.im2col_encoding(context_client, single[0][25:].tolist(), 2, embedding_size, 1) #24\n",
    "\n",
    "            x_enc3, windows_nb3 = ts.im2col_encoding(context_client, single[0][:27].tolist(), 3, embedding_size, 1) #25\n",
    "            x_enc4, windows_nb4 = ts.im2col_encoding(context_client, single[0][25:].tolist(), 3, embedding_size, 1) #23\n",
    "\n",
    "            x_enc5, windows_nb5 = ts.im2col_encoding(context_client, single[0][:27].tolist(), 5, embedding_size, 1) #23\n",
    "            x_enc6, windows_nb6 = ts.im2col_encoding(context_client, single[0][23:].tolist(), 5, embedding_size, 1) #23\n",
    "\n",
    "            cipher_out = []\n",
    "            fc_weight = self.fc._parameters['weight'].clone().T\n",
    "\n",
    "            for idx in range(len(self.convs)):\n",
    "                conv_weights = self.convs[idx][0]._parameters['weight'].tolist()\n",
    "                for channel in range(num_filters):\n",
    "                    kernel = conv_weights[channel][0]\n",
    "\n",
    "                    if len(kernel) == 2: # 2-gram, 25+24\n",
    "                        c_conv1 = x_enc1.conv2d_im2col(kernel, windows_nb1)\n",
    "                        c_conv2 = x_enc2.conv2d_im2col(kernel, windows_nb2)\n",
    "                        \n",
    "                        h1 = swish(c_conv1)\n",
    "                        h2 = swish(c_conv2)\n",
    "                        ap = h1.sum() + h2.sum() # [1, sequence_length - filter_size + 1]\n",
    "\n",
    "                        cipher_out.append(ap)\n",
    "                        print(\"Conv out: \", ap.decrypt())\n",
    "                    elif len(kernel) == 3: # 3-gram, 25+23\n",
    "                        c_conv1 = x_enc3.conv2d_im2col(kernel, windows_nb3)\n",
    "                        c_conv2 = x_enc4.conv2d_im2col(kernel, windows_nb4)\n",
    "\n",
    "                        h1 = swish(c_conv1)\n",
    "                        h2 = swish(c_conv2)\n",
    "                        ap = h1.sum() + h2.sum() # [1, sequence_length - filter_size + 1]\n",
    "\n",
    "                        cipher_out.append(ap)\n",
    "                        print(\"Conv out: \", ap.decrypt())\n",
    "                    elif len(kernel) == 5: # 5-gram, 23+23\n",
    "                        c_conv1 = x_enc5.conv2d_im2col(kernel, windows_nb5)\n",
    "                        c_conv2 = x_enc6.conv2d_im2col(kernel, windows_nb6)\n",
    "\n",
    "                        h1 = swish(c_conv1)\n",
    "                        h2 = swish(c_conv2)\n",
    "                        ap = h1.sum() + h2.sum() # [1, sequence_length - filter_size + 1]\n",
    "\n",
    "                        cipher_out.append(ap)\n",
    "                        print(\"Conv out: \", ap.decrypt())\n",
    "\n",
    "                    fc_weight[idx * num_filters + channel] /= sequence_length - len(kernel) + 1\n",
    "\n",
    "            cipher_out = ts.pack_vectors(cipher_out)\n",
    "\n",
    "            fc_bias = self.fc._parameters['bias']\n",
    "            cipher_out = cipher_out.mm_(fc_weight.tolist()) + fc_bias.tolist() # [1, num_classes]\n",
    "\n",
    "            print(\"FC out: \", cipher_out.decrypt())\n",
    "            cipher_logit = softmax(cipher_out)\n",
    "\n",
    "            cipher_logits.append(cipher_logit.decrypt())\n",
    "            print(\"Softmax out: \", cipher_logit.decrypt())\n",
    "        \n",
    "        acc_loss = (np.abs(np.array(cipher_logits) - np.array(plain_logit.tolist())) / np.array(plain_logit.tolist())).sum()\n",
    "        \n",
    "        print(\"Batch acc loss: \", acc_loss)\n",
    "        \n",
    "        return plain_logit\n",
    "    \n",
    "    def forward_plain(self, X):\n",
    "        embedded_chars = self.embedding(X)# [batch_size, sequence_length, sequence_length]\n",
    "        embedded_chars = embedded_chars.unsqueeze(1)\n",
    "\n",
    "        out = [conv(embedded_chars) for conv in self.convs]\n",
    "        out = torch.cat(out, dim=1)\n",
    "        out = out.view(embedded_chars.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        logit = self.sm(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(preds, y):\n",
    "    correct = torch.eq(preds, y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "train_set_size = 128\n",
    "\n",
    "def train(model, optimizer, criterion):\n",
    "    avg_acc = []\n",
    "    avg_loss = []\n",
    "    model.train()\n",
    "    for batch_idx , batch in enumerate(train_iter):\n",
    "        if batch_idx >= train_set_size:\n",
    "            continue\n",
    "        text, labels = batch.text , batch.label - 1\n",
    "        predicted = model.forward_plain(text)\n",
    "\n",
    "        acc = binary_acc(torch.max(predicted, dim=1)[1], labels)\n",
    "        avg_acc.append(acc)\n",
    "        loss = criterion(predicted, labels)\n",
    "        avg_loss.append(loss)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return np.array(avg_acc).mean()\n",
    "\n",
    "def evaluate(model, criterion):\n",
    "    avg_acc = []\n",
    "    model.eval()\n",
    "    for batch_idx , batch in enumerate(dev_iter):\n",
    "        text, labels = batch.text , batch.label - 1\n",
    "        predicted = model(text)\n",
    "\n",
    "        acc = binary_acc(torch.max(predicted, dim=1)[1], labels)\n",
    "        avg_acc.append(acc)\n",
    "\n",
    "    return np.array(avg_acc).mean()\n",
    "\n",
    "def evaluate_plain(model, criterion):\n",
    "    avg_acc = []\n",
    "    model.eval()\n",
    "    for batch_idx , batch in enumerate(dev_iter):\n",
    "        text, labels = batch.text , batch.label - 1\n",
    "        predicted = model.forward_plain(text)\n",
    "\n",
    "        acc = binary_acc(torch.max(predicted, dim=1)[1], labels)\n",
    "        avg_acc.append(acc)\n",
    "\n",
    "    return np.array(avg_acc).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HETextCNN(\n",
      "  (embedding): Embedding(14795, 50)\n",
      "  (convs): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1, 2, kernel_size=(2, 50), stride=(1, 1), bias=False)\n",
      "      (1): Swish()\n",
      "      (2): AvgPool2d(kernel_size=(49, 1), stride=(49, 1), padding=0)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(1, 2, kernel_size=(3, 50), stride=(1, 1), bias=False)\n",
      "      (1): Swish()\n",
      "      (2): AvgPool2d(kernel_size=(48, 1), stride=(48, 1), padding=0)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(1, 2, kernel_size=(5, 50), stride=(1, 1), bias=False)\n",
      "      (1): Swish()\n",
      "      (2): AvgPool2d(kernel_size=(46, 1), stride=(46, 1), padding=0)\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=6, out_features=2, bias=True)\n",
      "  (sm): Softmax()\n",
      ")\n",
      "epoch=0,训练准确率=0.54296875\n",
      "epoch=0,测试准确率=0.5122767686843872\n",
      "epoch=1,训练准确率=0.56103515625\n",
      "epoch=1,测试准确率=0.5982142686843872\n",
      "epoch=2,训练准确率=0.6494140625\n",
      "epoch=2,测试准确率=0.6875\n",
      "epoch=3,训练准确率=0.705322265625\n",
      "epoch=3,测试准确率=0.7399553656578064\n",
      "epoch=4,训练准确率=0.770263671875\n",
      "epoch=4,测试准确率=0.7477678656578064\n",
      "epoch=5,训练准确率=0.783447265625\n",
      "epoch=5,测试准确率=0.7522321343421936\n",
      "epoch=6,训练准确率=0.814453125\n",
      "epoch=6,测试准确率=0.7589285969734192\n",
      "epoch=7,训练准确率=0.818115234375\n",
      "epoch=7,测试准确率=0.7667410969734192\n",
      "epoch=8,训练准确率=0.837890625\n",
      "epoch=8,测试准确率=0.78125\n",
      "epoch=9,训练准确率=0.85205078125\n",
      "epoch=9,测试准确率=0.7756696343421936\n",
      "epoch=10,训练准确率=0.860595703125\n",
      "epoch=10,测试准确率=0.7823660969734192\n",
      "epoch=11,训练准确率=0.857421875\n",
      "epoch=11,测试准确率=0.7991071343421936\n",
      "epoch=12,训练准确率=0.855224609375\n",
      "epoch=12,测试准确率=0.7935267686843872\n",
      "epoch=13,训练准确率=0.85986328125\n",
      "epoch=13,测试准确率=0.7823660969734192\n",
      "epoch=14,训练准确率=0.86328125\n",
      "epoch=14,测试准确率=0.8013392686843872\n",
      "epoch=15,训练准确率=0.876708984375\n",
      "epoch=15,测试准确率=0.7868303656578064\n",
      "epoch=16,训练准确率=0.8779296875\n",
      "epoch=16,测试准确率=0.7901785969734192\n",
      "epoch=17,训练准确率=0.884033203125\n",
      "epoch=17,测试准确率=0.7790178656578064\n",
      "epoch=18,训练准确率=0.88916015625\n",
      "epoch=18,测试准确率=0.7845982313156128\n",
      "epoch=19,训练准确率=0.887451171875\n",
      "epoch=19,测试准确率=0.7868303656578064\n",
      "epoch=20,训练准确率=0.8896484375\n",
      "epoch=20,测试准确率=0.8035714030265808\n",
      "epoch=21,训练准确率=0.895263671875\n",
      "epoch=21,测试准确率=0.7767857313156128\n",
      "epoch=22,训练准确率=0.886474609375\n",
      "epoch=22,测试准确率=0.8091517686843872\n",
      "epoch=23,训练准确率=0.897705078125\n",
      "epoch=23,测试准确率=0.7946428656578064\n",
      "epoch=24,训练准确率=0.89501953125\n",
      "epoch=24,测试准确率=0.8069196343421936\n",
      "epoch=25,训练准确率=0.9033203125\n",
      "epoch=25,测试准确率=0.7946428656578064\n",
      "epoch=26,训练准确率=0.896728515625\n",
      "epoch=26,测试准确率=0.796875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-62f6283e3031>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'epoch={},训练准确率={}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_plain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-50d4f6449e15>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, criterion)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mavg_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\syft\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\syft\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'betas'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m             F.adam(params_with_grad,\n\u001b[0m\u001b[0;32m    109\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\syft\\lib\\site-packages\\torch\\optim\\functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = HETextCNN()\n",
    "print(model)\n",
    "\n",
    "pretrained_embedding = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embedding)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_accs, test_accs = [], []\n",
    "\n",
    "for epoch in range(50):\n",
    "\n",
    "    train_acc = train(model, optimizer, criterion)\n",
    "    print('epoch={},训练准确率={}'.format(epoch, train_acc))\n",
    "    test_acc = evaluate_plain(model, criterion)\n",
    "    print(\"epoch={},测试准确率={}\".format(epoch, test_acc))\n",
    "test_acc = evaluate(model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
